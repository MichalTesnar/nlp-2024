{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0756002729d71",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "N-gram models are a simple and effective method for predicting the next item in a sequence. An n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application. N-gram models are widely used in natural language processing (NLP) tasks such as text prediction, spell checking, and speech recognition. They work by calculating the probability of each word in a sentence based on the occurrence of its preceding words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e2a1eb5410e03",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Mathematical Background\n",
    "The probability of a word sequence in an n-gram model is given by the conditional probability of each word given the previous words. For a bigram model (2-gram), the probability of the word sequence \"w1 w2 w3\" is calculated as P(w3|w2) * P(w2|w1) * P(w1). Smoothing techniques, like Laplace smoothing, are used to handle cases where the probability of a word sequence is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610291cbe7684a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da478bb8f667e095",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T18:04:01.491417Z",
     "start_time": "2024-02-13T18:03:57.917998Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/michal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/michal/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure NLTK is installed:\n",
    "# !pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk.corpus\n",
    "import random as rnd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86abe60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'for',\n",
       " 'n-gram',\n",
       " 'modeling',\n",
       " 'n-gram',\n",
       " 'models',\n",
       " 'are',\n",
       " 'useful',\n",
       " 'and',\n",
       " 'n-gram',\n",
       " 'models',\n",
       " 'are',\n",
       " 'useful']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# sample_text is converted to a list of strings\n",
    "# Feel free to try different texts!\n",
    "sample_text = \"This is a sample text for n-gram modeling. N-gram models are useful and n-gram models are useful.\"\n",
    "tokens = sample_text.lower().split()\n",
    "# remove periods\n",
    "tokens = [token.replace(\".\", \"\") for token in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3c9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the n-gram model\n",
    "# Takes n as the number of consecutive words to be considered\n",
    "# Keeps track of the bigrams as well as their occurence\n",
    "def n_gram_model(n, sentences):\n",
    "    n_grams = defaultdict(int)\n",
    "    for sentence in tqdm(sentences): #tqdm shows progress bar\n",
    "        words = [word for word in sentence]\n",
    "        for i in range(len(words) - n + 1):\n",
    "            n_grams[tuple(words[i:i + n])] += 1\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57773b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4733.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Note that the function expects a list of sentences. In this case it is a list of 1 sentence\n",
    "bigram_model = n_gram_model(2, [tokens]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e6ea08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('this', 'is'): 1,\n",
       "             ('is', 'a'): 1,\n",
       "             ('a', 'sample'): 1,\n",
       "             ('sample', 'text'): 1,\n",
       "             ('text', 'for'): 1,\n",
       "             ('for', 'n-gram'): 1,\n",
       "             ('n-gram', 'modeling'): 1,\n",
       "             ('modeling', 'n-gram'): 1,\n",
       "             ('n-gram', 'models'): 2,\n",
       "             ('models', 'are'): 2,\n",
       "             ('are', 'useful'): 2,\n",
       "             ('useful', 'and'): 1,\n",
       "             ('and', 'n-gram'): 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b0bac1d742952",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Easy Example: Building a Simple Text Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4258044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(n_gram_model, word):\n",
    "    word = tuple([word])\n",
    "    candidates = [(key[-1], count) for key, count in n_gram_model.items() if key[:-1] == word] # go till the last last but one\n",
    "    if candidates:\n",
    "        return max(candidates, key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34aa7891d1aa0c85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T18:05:54.870485Z",
     "start_time": "2024-02-13T18:05:54.864369Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# Test the predictor\n",
    "print(predict_next_word(bigram_model, 'is'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235364110b31cae0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Example: Building and Evaluating a Trigram Model\n",
    "Objective:\n",
    "Implement a trigram (3-gram) model using the Brown Corpus, and then use this model to generate new sentences. Finally, evaluate its performance by calculating the perplexity of a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bcfea",
   "metadata": {},
   "source": [
    "### Part 1: Building the Trigram Model\n",
    "You should first retrieve the data and split it into training and testing. You may choose to implement preprocessing steps such as converting to lowercase or removing punctuation. Then, build your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d997ea26667085f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T18:20:34.463772Z",
     "start_time": "2024-02-13T18:20:33.006208Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/michal/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/michal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# (Down)load the Brown Corpus\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')  # For sentence tokenization\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "838f18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the brown corpus\n",
    "sentences = nltk.corpus.brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cdd0b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', 'atlantas', 'recent', 'primary', 'election', 'produced', '', 'no', 'evidence', '', 'that', 'any', 'irregularities', 'took', 'place', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Optional preprocessing examples\n",
    "# Make sure to use the appropriate variable names if you choose to use these\n",
    "\n",
    "# Convert to lowercase:\n",
    "lowercase_sentences = [[word.lower() for word in sentence] for sentence in sentences]\n",
    "print(lowercase_sentences[0])\n",
    "\n",
    "# Remove punctuation with regular expression\n",
    "pattern = re.compile(r'[^\\w\\s]')\n",
    "\n",
    "# Convert sentences to lowercase and remove punctuation\n",
    "cleaned_sentences = [[pattern.sub('', word) for word in sentence] for sentence in lowercase_sentences]\n",
    "print(cleaned_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a837d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and end tokens to each sentence\n",
    "# There should be n-1 start tokens\n",
    "# We will be working with trigrams, so two start tokens are needed\n",
    "sentences_with_tokens = [['<start>'] + ['<start>'] + sentence + ['<end>'] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cf1f5cb75869053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T18:21:03.882462Z",
     "start_time": "2024-02-13T18:21:03.874329Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle and split the data\n",
    "import random\n",
    "# Shuffle the sentences\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(sentences_with_tokens)\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "split_idx = int(len(sentences_with_tokens) * 0.8)\n",
    "train_sentences = sentences_with_tokens[:split_idx]\n",
    "test_sentences = sentences_with_tokens[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f47dcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45872/45872 [00:00<00:00, 67591.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "n = 3\n",
    "trigram_model = n_gram_model(n, train_sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd0ddc890e8c8f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 2: Generating New Sentences\n",
    "Use your trigram model to generate a sentence. Start with a seed of two words and then use the model to find the next word with the highest count. Append this word to the sentence, and then use the last two words of the sentence as the new seed. Repeat this process until you generate an end token (\\</s>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf269fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(n, n_grams, seed=None, n_words=10):\n",
    "    sentence = []\n",
    "    \n",
    "    # Start the sentence with <start> tokens\n",
    "    for _ in range(n-1):\n",
    "        sentence.append(\"<start>\")\n",
    "\n",
    "    for i in range(n_words):\n",
    "        if i == 0 and seed != None:\n",
    "            sentence.extend(seed) # add seed to sentence\n",
    "        else:\n",
    "            last_n_minus_1_words = tuple(sentence[-(n-1):])  # Consider the last (n-1) words\n",
    "            if \"<end>\" in last_n_minus_1_words:  # Stop generating if <end> token encountered\n",
    "                break\n",
    "            \n",
    "            next_word_candidates = [key[-1] for key in n_grams if key[:-1] == last_n_minus_1_words]\n",
    "            if next_word_candidates: # Check if the previous word is in the model \n",
    "                next_word_counts = [n_grams[key] for key in n_grams if key[:-1] == last_n_minus_1_words]\n",
    "                next_word = next_word_candidates[next_word_counts.index(max(next_word_counts))]\n",
    "                sentence.append(next_word)\n",
    "            else:\n",
    "                break  # If no matching n-grams, stop generating the sentence\n",
    "\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61a7ba90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> <start> This is a good deal of pleasure ; ; <end>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(n, trigram_model, seed=[\"This\", \"is\"], n_words=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a20be",
   "metadata": {},
   "source": [
    "The generate_sentence() function chooses the word with the highest count\n",
    "You may want to implement the function such that it turns the counts into probabilities instead\n",
    "i.e. the higher the count, the higher the probability of selecting the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f276fd5303633",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 3: Evaluating the Model (Perplexity Calculation)\n",
    "Perplexity is a common metric to evaluate language models. This implements a function to calculate the perplexity of the trigram model on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38a2db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_perplexity(n, test_data, n_grams):\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for sentence in tqdm(test_data):\n",
    "        for i in range(n - 1, len(sentence)):\n",
    "            full_context = tuple(sentence[i - n + 1:i + 1]) # Context + current word\n",
    "            context = tuple(sentence[i - n + 1:i])  # Context for the current word\n",
    "\n",
    "            # Calculate the probability of the current word given the context\n",
    "            if n_grams[full_context]: # Check if the sequence of n words exists in the model\n",
    "                next_word_counts = [n_grams[key] for key in n_grams if key[:-1] == context]\n",
    "                curr_word_prob = n_grams[full_context] / sum(next_word_counts)                \n",
    "                total_log_prob += math.log(curr_word_prob)\n",
    "            else: # if the curr_word was not encountered, assign a small probability\n",
    "                prob = 1e-6 # can be adjusted based on your dataset\n",
    "                total_log_prob += prob\n",
    "            \n",
    "            total_words += 1\n",
    "\n",
    "    avg_log_prob = total_log_prob / total_words\n",
    "    perplexity = math.exp(-avg_log_prob)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04008a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a subset of the test set as this runs pretty slowly\n",
    "print(\"Perplexity:\", compute_perplexity(n, random.sample(test_sentences, 100), trigram_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
